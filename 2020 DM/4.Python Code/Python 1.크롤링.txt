from bs4 import BeautifulSoup
from selenium import webdriver
import pandas as pd 
import datetime

driver = webdriver.Chrome("C:/Users/황윤섭/Desktop/chromedriver")
delay_time = 3
driver.implicitly_wait(delay_time)

driver.get('https://nid.naver.com/nidlogin.login')

K=list()


a=0      
A1=''
A2=''
B1=''
B2=''
C1=''
C2=''
D1=''
D2=''
K2=''
K3=''


date1=datetime.date(2019,1,1)
abc=[date1]


for i in range(0,365):
    date1=date1+datetime.timedelta(days=1)
    abc.append(date1)


   
for i in abc:
    
    
        date11=str(i)
        date22=str(i)
        year1=date11[0:4]
        year2=date22[0:4]
        month1=date11[5:7]
        month2=date22[5:7]
        day1=date11[8:]
        day2=date22[8:]
        
        Q=True
        i=1
        
        URL='https://kin.naver.com/search/list.nhn?sort=none&query=%EC%A4%913%20-%EB%8F%88%20-%EA%B3%B5%EB%B6%80%20-%EC%88%98%ED%95%99%20-%EC%98%81%EC%96%B4%20-%EA%B3%BC%ED%95%99%20-%EC%8B%9C%ED%97%98%20-%EC%9D%B8%EA%B0%95%20-%EA%B0%9C%ED%95%99%20-%EC%A7%84%ED%95%99%20-%EC%B1%85%20-%EB%A0%8C%EC%A6%88%20-%EC%B9%B4%EB%93%9C%20-%EC%84%A0%EB%AC%BC%20-%EA%B2%8C%EC%9E%84%20-%ED%95%99%EC%9B%90%20-%EC%88%98%EC%97%85&period='+year1+'.'+month1+'.'+day1+'.%7C'+year2+'.'+month2+'.'+day2+'.&section=kin&page='
        
        while(Q==True):
            
            NewURL=URL+str(i)
            driver.get(NewURL)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            v= str(soup.select('div.section > h2 > span > em '))[9:12]
            q= str(soup.select('div.section > h2 > span > em '))[13:16]
            
            f= str(soup.select('div.section > h2 > span > em '))[8:10]
            g= str(soup.select('div.section > h2 > span > em '))[11:13]
            u= str(soup.select('div.section > h2 > span > em '))[13]
                        
            
            
            for href in soup.find("ul", class_="basic1").find_all("dt"):
                driver.get(href.find("a")["href"])
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                
                A = soup.select('div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__title > div > div')
                for list in A:
                    A2+=list.text.strip()
                    replaceA2= A2.replace(",","")
                    A1+=replaceA2
                    
                if(A1==''):
                    t=soup.select('div.question-content > div > div.c-heading._questionContentsArea.c-heading--multiple > div.c-heading__title > div > div.title')
                    for list in t:
                        K2+=list.text.strip()
                        replaceK2=K2.replace(",","")
                        A1+=replaceK2
                if(A1==''):
                    r=soup.select('div.question-content > div > div.c-heading._questionContentsArea.c-heading--default > div.c-heading__title > div > div.title')
                    for list in r:
                        K3+=list.text.strip()
                        replaceK3=K3.replace(",","")
                        A1+=replaceK3
                            
                    
                    
                B = soup.select('div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__content')
                for list in B:
                    B2+=list.text.strip()
                    replaceB2= B2.replace(",","")
                    B1+=replaceB2    
                    
                    
                C = soup.select('div.question-content > div > div.tag-list.tag-list--end-title > a.tag-list__item.tag-list__item--category')
                for list in C:
                    C2+=list.text.strip()
                    replaceC2= C2.replace(",","")
                    C1+=replaceC2
                
                D = soup.select('div.question-content > div > div.c-userinfo > div.c-userinfo__left > span')
                for list in D:
                    D2+=list.text.strip()
                    replaceD2= D2.replace(",","")
                    D1+=replaceD2                
                D1=D1[3:13]
                
                
                K1=A1+','+B1+','+C1+','+D1    
                K.append(K1)
               
  

                A1=''
                A2=''
                B1=''
                B2=''
                C1=''
                C2=''       
                D1=''
                D2=''
                K2=''
                K3=''
            

            if(v==q):
                Q=False
            
            if(f==g and u=='<'):
                Q=False
                
            i+=1    
                
                

df = pd.DataFrame(K)    
df1=df[0].str.split(',', expand=True)
df1.columns=['Title','Text','Category','Date']
for i in range(0,len(df1.index.values)):
         df1['Category'][i]=df1['Category'][i].replace('태그 디렉터리Ξ','')
        
df1.to_excel('C:/Users/황윤섭/Desktop/중3 크롤링.xlsx')         